{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Visualizing the learning of a neural network\n","\n","#### prerequisite \n","* Download [tiny-imagenet-challange](https://www.kaggle.com/competitions/tiny-imagenet/overview) data\n","* Install dependencies "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T07:25:37.521298Z","iopub.status.busy":"2022-12-13T07:25:37.521008Z","iopub.status.idle":"2022-12-13T07:25:37.527169Z","shell.execute_reply":"2022-12-13T07:25:37.526130Z","shell.execute_reply.started":"2022-12-13T07:25:37.521273Z"},"trusted":true},"outputs":[],"source":["#imports\n","import torch\n","import torchvision \n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot  as plt\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torch.nn as nn\n","import time\n","from tqdm import tqdm\n","import os"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T07:25:42.282391Z","iopub.status.busy":"2022-12-13T07:25:42.282003Z","iopub.status.idle":"2022-12-13T07:25:44.632678Z","shell.execute_reply":"2022-12-13T07:25:44.630607Z","shell.execute_reply.started":"2022-12-13T07:25:42.282359Z"},"trusted":true},"outputs":[{"data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["vgg_16 = torchvision.models.vgg16()\n","vgg_16"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-13T07:27:41.980671Z","iopub.status.busy":"2022-12-13T07:27:41.980215Z","iopub.status.idle":"2022-12-13T07:27:47.894541Z","shell.execute_reply":"2022-12-13T07:27:47.893679Z","shell.execute_reply.started":"2022-12-13T07:27:41.980625Z"},"trusted":true},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","VGG                                      [1, 1000]                 --\n","├─Sequential: 1-1                        [1, 512, 2, 2]            --\n","│    └─Conv2d: 2-1                       [1, 64, 64, 64]           1,792\n","│    └─ReLU: 2-2                         [1, 64, 64, 64]           --\n","│    └─Conv2d: 2-3                       [1, 64, 64, 64]           36,928\n","│    └─ReLU: 2-4                         [1, 64, 64, 64]           --\n","│    └─MaxPool2d: 2-5                    [1, 64, 32, 32]           --\n","│    └─Conv2d: 2-6                       [1, 128, 32, 32]          73,856\n","│    └─ReLU: 2-7                         [1, 128, 32, 32]          --\n","│    └─Conv2d: 2-8                       [1, 128, 32, 32]          147,584\n","│    └─ReLU: 2-9                         [1, 128, 32, 32]          --\n","│    └─MaxPool2d: 2-10                   [1, 128, 16, 16]          --\n","│    └─Conv2d: 2-11                      [1, 256, 16, 16]          295,168\n","│    └─ReLU: 2-12                        [1, 256, 16, 16]          --\n","│    └─Conv2d: 2-13                      [1, 256, 16, 16]          590,080\n","│    └─ReLU: 2-14                        [1, 256, 16, 16]          --\n","│    └─Conv2d: 2-15                      [1, 256, 16, 16]          590,080\n","│    └─ReLU: 2-16                        [1, 256, 16, 16]          --\n","│    └─MaxPool2d: 2-17                   [1, 256, 8, 8]            --\n","│    └─Conv2d: 2-18                      [1, 512, 8, 8]            1,180,160\n","│    └─ReLU: 2-19                        [1, 512, 8, 8]            --\n","│    └─Conv2d: 2-20                      [1, 512, 8, 8]            2,359,808\n","│    └─ReLU: 2-21                        [1, 512, 8, 8]            --\n","│    └─Conv2d: 2-22                      [1, 512, 8, 8]            2,359,808\n","│    └─ReLU: 2-23                        [1, 512, 8, 8]            --\n","│    └─MaxPool2d: 2-24                   [1, 512, 4, 4]            --\n","│    └─Conv2d: 2-25                      [1, 512, 4, 4]            2,359,808\n","│    └─ReLU: 2-26                        [1, 512, 4, 4]            --\n","│    └─Conv2d: 2-27                      [1, 512, 4, 4]            2,359,808\n","│    └─ReLU: 2-28                        [1, 512, 4, 4]            --\n","│    └─Conv2d: 2-29                      [1, 512, 4, 4]            2,359,808\n","│    └─ReLU: 2-30                        [1, 512, 4, 4]            --\n","│    └─MaxPool2d: 2-31                   [1, 512, 2, 2]            --\n","├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n","├─Sequential: 1-3                        [1, 1000]                 --\n","│    └─Linear: 2-32                      [1, 4096]                 102,764,544\n","│    └─ReLU: 2-33                        [1, 4096]                 --\n","│    └─Dropout: 2-34                     [1, 4096]                 --\n","│    └─Linear: 2-35                      [1, 4096]                 16,781,312\n","│    └─ReLU: 2-36                        [1, 4096]                 --\n","│    └─Dropout: 2-37                     [1, 4096]                 --\n","│    └─Linear: 2-38                      [1, 1000]                 4,097,000\n","==========================================================================================\n","Total params: 138,357,544\n","Trainable params: 138,357,544\n","Non-trainable params: 0\n","Total mult-adds (G): 1.38\n","==========================================================================================\n","Input size (MB): 0.05\n","Forward/backward pass size (MB): 8.92\n","Params size (MB): 553.43\n","Estimated Total Size (MB): 562.40\n","=========================================================================================="]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# ! pip install torchinfo\n","\n","from torchinfo import summary\n","summary(vgg_16, input_size=(1,3,64,64))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_ds_path = \"../input/tiny-imagenet-challenge/TinyImageNet/train\"\n","torch.manual_seed(0)\n","tiny_image_net_dataset = datasets.ImageFolder(train_ds_path, \n","                                              transform = transforms.Compose([transforms.ToTensor(),\n","                                              transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n","                                                                             ]))\n","tiny_image_net_dataloader = DataLoader(tiny_image_net_dataset, batch_size=512, shuffle=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["image, label =  next(iter(tiny_image_net_dataloader))\n","i = 0\n","plt.title(f\"{label[i]}\")\n","plt.imshow(image[i].permute(1,2,0))\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","vgg_16 = vgg_16.to(device)\n","\n","crt_CEL = nn.CrossEntropyLoss()\n","\n","optm_vgg_16 = optim.SGD(vgg_16.parameters(), lr=0.001, momentum=0.9)\n","\n","sch_cycl_lr = lr_scheduler.CyclicLR(optm_vgg_16,base_lr=0.001,max_lr=0.01, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","    best_acc = 0.0\n","    model_dir = 'model/vgg16/'\n","#     os.makedirs(model_dir)\n","    for epoch in range(num_epochs):\n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print('-' * 10)\n","      \n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        batch = 0 \n","        for i ,(inputs, labels) in enumerate (tqdm(tiny_image_net_dataloader)):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","            scheduler.step()\n","\n","\n","\n","        epoch_loss = running_loss / len(tiny_image_net_dataset)\n","        epoch_acc = running_corrects.double() / len(tiny_image_net_dataset)\n","\n","\n","\n","        weights = dict()    \n","        for param in parameters :\n","                weights[param] = model.state_dict()[param]  \n","\n","        torch.save({\"Model_params\":weights ,\n","                   \"EPOCH\":epoch,\n","                   \"Loss\":epoch_loss},f\"{model_dir}/vgg16_{epoch}.pt\")\n","        print(f' Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n","\n","\n","       \n","           \n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n","    print(f'Best val Acc: {best_acc:4f}')\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train(model=vgg_16, criterion=crt_CEL, optimizer=optm_vgg_16, scheduler=sch_cycl_lr, num_epochs=64)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["parameters = [\"features.0.weight\",\"features.2.weight\",\"features.5.weight\",\"features.7.weight\",\"features.10.weight\",             \n","                \"features.12.weight\",\"features.14.weight\",\"features.17.weight\",\"features.19.weight\",\"features.21.weight\",\n","                \"features.24.weight\",\"features.26.weight\", \"features.28.weight\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T17:53:51.879648Z","iopub.status.busy":"2022-12-11T17:53:51.879264Z","iopub.status.idle":"2022-12-11T17:53:51.927446Z","shell.execute_reply":"2022-12-11T17:53:51.926450Z","shell.execute_reply.started":"2022-12-11T17:53:51.879615Z"},"trusted":true},"outputs":[],"source":["vgg16_wts = torch.load(\"model/vgg16/vgg16_0.pt\")[\"Model_params\"]\n","# print(model_params[parameters[-1]].shape)\n","i = 0\n","layer_i_sum = torch.sum(vgg16_wts[parameters[i]],dim=1) \n","upscale = torch.nn.functional.interpolate(torch.unsqueeze(layer_i_sum, dim=1),\n","                                          size = (64,64),mode='nearest').cpu()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T17:57:54.264112Z","iopub.status.busy":"2022-12-11T17:57:54.263495Z","iopub.status.idle":"2022-12-11T17:59:23.725863Z","shell.execute_reply":"2022-12-11T17:59:23.724798Z","shell.execute_reply.started":"2022-12-11T17:57:54.264072Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","for wts_file in sorted(os.listdir(\"model/vgg16\"), key = lambda x : int(x[6:-3])):\n","    vgg16_wts = torch.load(f\"model/vgg16/{wts_file}\")[\"Model_params\"]\n","#     print(wts_file)\n","    for parameter in parameters :\n","        layer_i_sum = torch.sum(vgg16_wts[parameter], dim=1) \n","        upscale = torch.nn.functional.interpolate(torch.unsqueeze((layer_i_sum ), dim=1),\n","                                                  size=(128,128), mode='nearest').cpu()\n","        for i in range(len(upscale)) :\n","            p = f\"model/vgg16_imgs/{parameter}/{i}/\"\n","            Path(p).mkdir(parents=True, exist_ok=True )\n","            cv2.imwrite(f\"{p}/{str(wts_file[6:-3]).zfill(2)}.png\",\n","                        ((upscale[i]+1)*127).permute(1,2,0).numpy())\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:08:13.430394Z","iopub.status.busy":"2022-12-11T18:08:13.429987Z","iopub.status.idle":"2022-12-11T18:14:18.419821Z","shell.execute_reply":"2022-12-11T18:14:18.418819Z","shell.execute_reply.started":"2022-12-11T18:08:13.430358Z"},"trusted":true},"outputs":[],"source":["import glob\n","from PIL import Image\n","def make_gif(frame_folder, name):\n","    frames = [Image.open(image) for image in glob.glob(f\"{frame_folder}/*\")]\n","    frame_one = frames[0]\n","    frame_one.save(name, format=\"GIF\", append_images=frames,\n","               save_all=True, duration=512, loop=1)\n","    \n","# for ft in parameters :\n","#     for     \n","#         make_gif(f\"model/vgg16_imgs/{ft}/{layer}/\", f\"model/gif/{ft.replace('.','')}_{layer}.gif\")    \n","           \n","    \n","for ft in os.listdir(\"model/vgg16_imgs/\")   :\n","    for layer in os.listdir(f\"model/vgg16_imgs/{ft}\") :\n","            make_gif(f\"model/vgg16_imgs/{ft}/{layer}/\", f\"model/gif/{ft.replace('.','')}_{layer}.gif\")    \n","          "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
