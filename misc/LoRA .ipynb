{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b56a85-52b4-4f7d-a6d7-39f403df55b6",
   "metadata": {},
   "source": [
    "## LoRA (Low Rank Adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94554ff0-20e0-42c3-a80d-97cc0682a4c6",
   "metadata": {},
   "source": [
    "##### Build the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0347eaf-3df4-44ce-b7d4-442a06fdc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir ~/.kaggle\n",
    "# ! cp kaggle.json ~/.kaggle/\n",
    "# ! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da331da-9f87-4b3b-9029-184c50b6e2a9",
   "metadata": {},
   "source": [
    "We will use the python questions dataset from [Kaggle](https://www.kaggle.com/datasets/stackoverflow/pythonquestions). To train a Transformer from [Bigcode](https://huggingface.co/bigcode/starcoderbase-1b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dce000-f55e-4061-89b2-28ff0987d257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! kaggle datasets download stackoverflow/pythonquestions\n",
    "# ! unzip pythonquestions.zip -d ./pythonquestions\n",
    "# ! pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1d58f1-f968-4619-81f9-0f8e078b12ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c01b90a-2fb4-41a6-9b20-e6b29496f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonquestions = \"./pythonquestions\"\n",
    "questions_df = pd.read_csv(f\"{pythonquestions}/Questions.csv\", encoding = 'ISO-8859-1')\n",
    "answers_df = pd.read_csv(f\"{pythonquestions}/Answers.csv\", encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6d154a-b3c6-428d-a25a-52b7e6e594e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>&lt;p&gt;open up a terminal (Applications-&amp;gt;Utilit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>&lt;p&gt;I haven't been able to find anything that d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>&lt;p&gt;Unfortunately the only API that isn't depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469</td>\n",
       "      <td>&lt;p&gt;I am using the Photoshop's javascript API t...</td>\n",
       "      <td>&lt;p&gt;There must be a method in Cocoa to get a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>502</td>\n",
       "      <td>&lt;p&gt;I have a cross-platform (Python) applicatio...</td>\n",
       "      <td>&lt;p&gt;You can use ImageMagick's convert utility f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   QuestionId                                           Question  \\\n",
       "0         469  <p>I am using the Photoshop's javascript API t...   \n",
       "1         469  <p>I am using the Photoshop's javascript API t...   \n",
       "2         469  <p>I am using the Photoshop's javascript API t...   \n",
       "3         469  <p>I am using the Photoshop's javascript API t...   \n",
       "4         502  <p>I have a cross-platform (Python) applicatio...   \n",
       "\n",
       "                                              Answer  \n",
       "0  <p>open up a terminal (Applications-&gt;Utilit...  \n",
       "1  <p>I haven't been able to find anything that d...  \n",
       "2  <p>Unfortunately the only API that isn't depre...  \n",
       "3  <p>There must be a method in Cocoa to get a li...  \n",
       "4  <p>You can use ImageMagick's convert utility f...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.rename(columns={\"Id\":\"QuestionId\"}, inplace=True)\n",
    "answers_df.rename(columns={\"ParentId\":\"QuestionId\"}, inplace=True)\n",
    "\n",
    "QnA_df = pd.merge(questions_df, answers_df, on = [\"QuestionId\"], suffixes=(\"_q\", \"_a\"))\n",
    "QnA_df = QnA_df[[\"QuestionId\", \"Body_q\", \"Body_a\"]]\n",
    "QnA_df.rename(columns={\"Body_q\":\"Question\", \"Body_a\":\"Answer\"}, inplace=True)\n",
    "\n",
    "QnA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3363fcb4-f1af-4fe3-8599-90f15f865ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/l/lib/python3.11/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(QnA_df).train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b434301-3beb-49e5-85bc-e1b4293c8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-1b\"\n",
    "device = \"cuda\" \n",
    "token=\"<HF_TOKEN>\"\n",
    "cache_dir=\"./model_cache/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, token=token, cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b09e33-5ca8-4d63-bae7-70b45d415655",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cd35a5c-4d8d-4557-8fde-60c129fb5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def process_html_text(batch):\n",
    "    \n",
    "    q = []\n",
    "    a = []\n",
    "    \n",
    "    for b in batch[\"Question\"]:   \n",
    "        q.append(BeautifulSoup(b, \"html\").text)\n",
    "    \n",
    "    for b in batch[\"Answer\"]:   \n",
    "        a.append(BeautifulSoup(b, \"html\").text)\n",
    "    \n",
    "    model_inputs = tokenizer(q, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(text_target=a, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4426226-92fa-4e3a-b62c-cccf15c23008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t_dataset = dataset[\"test\"].map(process_html_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be81d460-1722-4000-94a7-6bc615f4e1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 296137\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset = t_dataset.remove_columns([\"QuestionId\",\"Answer\",\"Question\"])\n",
    "t_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b81ed4-bb6f-482a-8f27-4e2514b4185e",
   "metadata": {},
   "source": [
    "**Store the processed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e92e4e-fbd2-4ed2-b39e-befe0d1a52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataset.save_to_disk(\"./pythonquestions/processed_ids.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42b9ab-48a6-48c3-85d0-d8cf4fe01633",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Load processed data, input_ids and Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fe17ee8-667d-4c4c-93b3-56e65d7181c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadb1c33eb4f46cdb4860498f4030cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/296137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "t_dataset = Dataset.load_from_disk(\"./pythonquestions/processed_ids.parquet\")\n",
    "t_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b47728-4c10-4727-aae7-810d606e2abc",
   "metadata": {},
   "source": [
    "##### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f24afc-96fd-418c-b4e1-4286b9eb2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-1b\"\n",
    "device = \"cuda\" \n",
    "token=\"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\"\n",
    "cache_dir=\"./model_cache/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e5413-3580-4b0c-b34c-5450acf8551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"rouge\", \"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bb3ce-e527-4ac3-a4d9-db1cb2351eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69883f5e-7b21-4108-83a6-27f9671ee160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "├─GPTBigCodeModel: 1-1                             --\n",
      "|    └─Embedding: 2-1                              100,663,296\n",
      "|    └─Embedding: 2-2                              16,777,216\n",
      "|    └─Dropout: 2-3                                --\n",
      "|    └─ModuleList: 2-4                             --\n",
      "|    |    └─GPTBigCodeBlock: 3-1                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-2                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-3                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-4                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-5                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-6                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-7                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-8                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-9                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-10                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-11                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-12                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-13                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-14                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-15                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-16                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-17                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-18                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-19                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-20                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-21                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-22                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-23                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-24                  42,490,112\n",
      "|    └─LayerNorm: 2-5                              4,096\n",
      "├─Linear: 1-2                                      100,663,296\n",
      "===========================================================================\n",
      "Total params: 1,237,870,592\n",
      "Trainable params: 1,237,870,592\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "├─GPTBigCodeModel: 1-1                             --\n",
       "|    └─Embedding: 2-1                              100,663,296\n",
       "|    └─Embedding: 2-2                              16,777,216\n",
       "|    └─Dropout: 2-3                                --\n",
       "|    └─ModuleList: 2-4                             --\n",
       "|    |    └─GPTBigCodeBlock: 3-1                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-2                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-3                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-4                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-5                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-6                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-7                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-8                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-9                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-10                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-11                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-12                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-13                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-14                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-15                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-16                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-17                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-18                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-19                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-20                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-21                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-22                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-23                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-24                  42,490,112\n",
       "|    └─LayerNorm: 2-5                              4,096\n",
       "├─Linear: 1-2                                      100,663,296\n",
       "===========================================================================\n",
       "Total params: 1,237,870,592\n",
       "Trainable params: 1,237,870,592\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f1b96-4097-49c8-bd37-04c7238730c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Trainer \n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"stackoverflowpython\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=t_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb5e16-ad39-48ea-86d6-a385b81749ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cc0eb-8d64-4242-9122-0b6e1ed27849",
   "metadata": {},
   "source": [
    "**OOM Error** Cannot load the model into GPU Memory.  \n",
    "\n",
    "Let's use LoRA to reduce the number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d19a522-5c84-4fe1-b123-21a9e69ba4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49152, 2048)\n",
       "    (wpe): Embedding(8192, 2048)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989319d-99af-48c2-9180-42806cc96788",
   "metadata": {},
   "source": [
    "##### LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4f6a55-2cf6-46fc-aee1-0769959071bf",
   "metadata": {},
   "source": [
    "**c_attn, c_proj, c_fc, wte, wpe** are the modules that have lot parameters that can be reduced to just **r** parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2b936-5aa5-4018-be6b-c4ee7122fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import copy\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"wte\", \"wpe\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "lora_model = get_peft_model(copy.deepcopy(model), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6192d8d9-a4ad-4c01-ac27-6550da1ccc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─LoraModel: 1-1                         --\n",
      "|    └─GPTBigCodeForCausalLM: 2-1        --\n",
      "|    |    └─GPTBigCodeModel: 3-1         (1,149,298,688)\n",
      "|    |    └─Linear: 3-2                  (100,663,296)\n",
      "=================================================================\n",
      "Total params: 1,249,961,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,249,961,984\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─LoraModel: 1-1                         --\n",
       "|    └─GPTBigCodeForCausalLM: 2-1        --\n",
       "|    |    └─GPTBigCodeModel: 3-1         (1,149,298,688)\n",
       "|    |    └─Linear: 3-2                  (100,663,296)\n",
       "=================================================================\n",
       "Total params: 1,249,961,984\n",
       "Trainable params: 0\n",
       "Non-trainable params: 1,249,961,984\n",
       "================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b2c25d-92f7-4711-8623-dc1e3b5ab4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTBigCodeForCausalLM(\n",
       "      (transformer): GPTBigCodeModel(\n",
       "        (wte): lora.Embedding(\n",
       "          (base_layer): Embedding(49152, 2048)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x49152])\n",
       "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 2048x16])\n",
       "        )\n",
       "        (wpe): lora.Embedding(\n",
       "          (base_layer): Embedding(8192, 2048)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 16x8192])\n",
       "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 2048x16])\n",
       "        )\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPTBigCodeBlock(\n",
       "            (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTBigCodeAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2304, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTBigCodeMLP(\n",
       "              (c_fc): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): PytorchGELUTanh()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287e925-6267-43c1-9cfe-b07f8a9c792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Trainer \n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"stackoverflowpython\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    lora_model,\n",
    "    training_args,\n",
    "    train_dataset=t_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf98b55-daa3-4405-8ddd-21afa6c05c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe90a2f0-7ed9-4067-a5dd-8284768746dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-1b\"\n",
    "device = \"cuda\" \n",
    "token=\"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\"\n",
    "cache_dir=\"./model_cache/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648784f4-2712-43b7-931b-1e1230c38a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  7 18:36:59 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   36C    P8              26W / 340W |     23MiB / 10240MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1659      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1902      G   /usr/bin/gnome-shell                          6MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a2532b-0d50-415f-90f1-f1d6e32a6b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTBigCodeForCausalLM(\n",
       "  (transformer): GPTBigCodeModel(\n",
       "    (wte): Embedding(49152, 2048)\n",
       "    (wpe): Embedding(8192, 2048)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPTBigCodeBlock(\n",
       "        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTBigCodeAttention(\n",
       "          (c_attn): Linear(in_features=2048, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTBigCodeMLP(\n",
       "          (c_fc): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (c_proj): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (act): PytorchGELUTanh()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1599bd3-6284-4caf-85ac-8bcc0ba8dfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  7 18:37:36 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   40C    P2              95W / 340W |   4646MiB / 10240MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1659      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1902      G   /usr/bin/gnome-shell                          6MiB |\n",
      "|    0   N/A  N/A   1630323      C   .../ubuntu/anaconda3/envs/l/bin/python     4618MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8351f593-67a5-449f-8771-53614fd30d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06c3f7d9-e331-4e27-b9c1-87211c305dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoderbase-1b\"\n",
    "device = \"cuda\" \n",
    "token=\"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\"\n",
    "cache_dir=\"./model_cache/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, trust_remote_code=True, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51410eec-df96-4090-87e7-5e6dcf6adb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import copy\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"wte\", \"wpe\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "lora_model = get_peft_model(copy.deepcopy(model), config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef147f3-f2c5-4478-82fd-eadc90546a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTBigCodeForCausalLM(\n",
       "      (transformer): GPTBigCodeModel(\n",
       "        (wte): lora.Embedding(\n",
       "          (base_layer): Embedding(49152, 2048)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 16x49152 (cuda:0)])\n",
       "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048x16 (cuda:0)])\n",
       "        )\n",
       "        (wpe): lora.Embedding(\n",
       "          (base_layer): Embedding(8192, 2048)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (lora_A): ModuleDict()\n",
       "          (lora_B): ModuleDict()\n",
       "          (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 16x8192 (cuda:0)])\n",
       "          (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 2048x16 (cuda:0)])\n",
       "        )\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x GPTBigCodeBlock(\n",
       "            (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTBigCodeAttention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2304, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTBigCodeMLP(\n",
       "              (c_fc): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act): PytorchGELUTanh()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=49152, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e92c02-d54c-46df-b39b-cb3c330a2b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb  7 18:43:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080        Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   40C    P2              95W / 340W |   4692MiB / 10240MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1659      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1902      G   /usr/bin/gnome-shell                          6MiB |\n",
      "|    0   N/A  N/A   1630974      C   .../ubuntu/anaconda3/envs/l/bin/python     4664MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27eccff0-8068-43bd-a9c2-a7ed964a798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "├─GPTBigCodeModel: 1-1                             --\n",
      "|    └─Embedding: 2-1                              100,663,296\n",
      "|    └─Embedding: 2-2                              16,777,216\n",
      "|    └─Dropout: 2-3                                --\n",
      "|    └─ModuleList: 2-4                             --\n",
      "|    |    └─GPTBigCodeBlock: 3-1                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-2                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-3                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-4                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-5                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-6                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-7                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-8                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-9                   42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-10                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-11                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-12                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-13                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-14                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-15                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-16                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-17                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-18                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-19                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-20                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-21                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-22                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-23                  42,490,112\n",
      "|    |    └─GPTBigCodeBlock: 3-24                  42,490,112\n",
      "|    └─LayerNorm: 2-5                              4,096\n",
      "├─Linear: 1-2                                      100,663,296\n",
      "===========================================================================\n",
      "Total params: 1,237,870,592\n",
      "Trainable params: 1,237,870,592\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "├─GPTBigCodeModel: 1-1                             --\n",
       "|    └─Embedding: 2-1                              100,663,296\n",
       "|    └─Embedding: 2-2                              16,777,216\n",
       "|    └─Dropout: 2-3                                --\n",
       "|    └─ModuleList: 2-4                             --\n",
       "|    |    └─GPTBigCodeBlock: 3-1                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-2                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-3                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-4                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-5                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-6                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-7                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-8                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-9                   42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-10                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-11                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-12                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-13                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-14                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-15                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-16                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-17                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-18                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-19                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-20                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-21                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-22                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-23                  42,490,112\n",
       "|    |    └─GPTBigCodeBlock: 3-24                  42,490,112\n",
       "|    └─LayerNorm: 2-5                              4,096\n",
       "├─Linear: 1-2                                      100,663,296\n",
       "===========================================================================\n",
       "Total params: 1,237,870,592\n",
       "Trainable params: 1,237,870,592\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c674cc5-0ae2-43db-a58b-21faf79f55f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─LoraModel: 1-1                         --\n",
      "|    └─GPTBigCodeForCausalLM: 2-1        --\n",
      "|    |    └─GPTBigCodeModel: 3-1         (1,149,298,688)\n",
      "|    |    └─Linear: 3-2                  (100,663,296)\n",
      "=================================================================\n",
      "Total params: 1,249,961,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,249,961,984\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─LoraModel: 1-1                         --\n",
       "|    └─GPTBigCodeForCausalLM: 2-1        --\n",
       "|    |    └─GPTBigCodeModel: 3-1         (1,149,298,688)\n",
       "|    |    └─Linear: 3-2                  (100,663,296)\n",
       "=================================================================\n",
       "Total params: 1,249,961,984\n",
       "Trainable params: 0\n",
       "Non-trainable params: 1,249,961,984\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "summary(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98ab55-50d0-4b6b-9d03-72ca5c03a0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l",
   "language": "python",
   "name": "l"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
